mlx>=0.26.0
numpy
transformers>=4.39.3
protobuf
pyyaml
jinja2

# Additional requirements for the inference server
# Install with: pip install -r requirements-inference-server.txt

# FastAPI framework
fastapi>=0.100.0
uvicorn[standard]>=0.23.0
aiohttp>=3.12.15

# File handling
python-multipart>=0.0.6

# Async support
aiofiles>=23.0.0

# For production deployment (optional)
gunicorn>=21.0.0